# Отчет

В этом файле лежит отчет о проделанной работе по соревнованию "Детекция мата в отзывах"

# Задача

Разработать модель, которая может автоматически находить нецензурные выражения в пользовательских отзывах на товары. Задача заключается не в замене или сокрытии этих слов, а в точном их выявлении.

## Датасет

Датасет состоит из набора строк вида:
- text: текстовый отзыв
- label: матерные слова *(на самом деле не слова)*, перечисленные через запятую, предварительно отсортированные в алфавитном порядке

244739 строк.

## Метрики

Метрикой качества является расстояние Левенштейна, замеренное между предсказанными и настоящими нецензурными выражениями.

# Датасет и препроцессинг

Т.к. формат данных у нас, можно сказать, Seq2Seq (Text2text), эту задачу можно решать 2 способами:

- Генерацией
- Дискриминативно

Рассмотрим 2 подхода:

## Генерация

Пускай формат данных и подразумевает возможное решение через генерацию, пользоваться ей в данном решении мы не будем, т.к. по изначальной постановке мы занимаемся именно детекцией, т.е. выявляем какие то данные, на основе уже полученных, никак не видоизменяя их. Есть риск получить на выходе данные, которые не соотносятся с нашим входом, пускай метрика у нас и расстояние Левенштейна, это было бы некорректно с точки зрения постановки задачи и в целом бизнеса. Дополнительно к этому, это очень тяжеловесное решение.

## Дискриминативное решение

Для решения задачи детекции нам нужно описать данную задачу как "Token Classification", где каждому токену мы будем ставить метку "мат"/"не мат".
Для этого необходимо преоборазовать наш датасет, где `target` это массив меток на каждый токен из исходного отзыва.

Здесь уже начинаются проблемы в данных и разметке, поскольку мы не имеем доступа к информации о том, как именно были размечены данные. Приведу примеры:

label:
- `гов..но`
- `полное говно, говнище`
- `гов... но`

В данном случае мы имеем проблемы со знаками препинания, по которым нужно разделять из-за структуры датасета. Но наибольшей проблемой тут является именно пробел, т.к. иногда словосочетания (или слова, которые по смыслу изначально не должны были быть разделены пробелом) оказываются размечены как один мат и становится непонятно, как потом такое словосочетание по предсказанным меткам токена собирать, нужно ли ставить между ними запятую или нет. Все современные способы токенизации (BPE, WordPiece и т.д.) подразумевают разделение по пробелу.

Приведу **пример**:

*(токенизация)* `Эта юбка полное гов... но` -> `эта`, `юб`, `#ка`, `полн`, `#ое`, `гов`, `#...`, `но`

*(предсказание)* -> [0, 0, 0, 1, 1, 1, 1, 1] -> `полное, гов..., но`, когда в нашем датасете это могло быть `полное гов... но`


Если бы у нас был другой датасет, в котором таких ситуаций не было или было бы оговорено, как именно размечаются данные, здесь проблемы бы не возникло. Я предположу, что во время разметки можно было выбрать любую подстроку и отметить как мат. Выход из такой ситуации есть - мы не будем никак предобрабатывать наши тексты, а в качестве токена возьмем самый маленький из возможных - любой символ.

Возможно в дальнейшем все же стоит попробовать это обратотать одним из способов и не обращать внимания на эти запятые.

# Модель
Мы предобработали наш датасет и получили массив токенов и массив лейблов к ним. Это также seq2seq задача, но есть ограничение: длины сиквенсов одинаковые. Значит в такой формулировке нам не нужна encoder-decoder архитектура. Воспользуемся классическим решением такой задачи: RNN

В качестве модели я выбрала Bidirectional LSTM.
Ембеддинги учим также свои. Т.к. словарь у нас маленький (~1000 символов, а наиболее встречающиеся, конечно, 33 буквы), поставим маленький embedding_size, а наибольшее количество параметров оставим именно в LSTM.

Также были опробованы модификации моделей Transformer Encoder, т.к. было предположение, что идея Attention хорошо подойдет для решения этой задачи, т.к. мы в действительности строим модель сильнее именно на взаимосвязях символов, чем на их отдельном смысле, т.к. наш токен - это один символ.

# Результаты


Попробовано 2 конфигурации BiLSTM:

- lstm_dim=128, num_layers=2, dropout=0.3, embedding_dim=32

Расстояние левенштейна на тесте: 0.35

- lstm_dim=256, num_layers=5, dropout=0.2, embedding_dim=16

Расстояние левенштейна на тесте: 0.33

На самом деле можно попробовать поставить даже меньше embedding_dim, для букв это довольно много.

А также конфигурации и разные попытки тренировки Transformer, но, т.к. модель сложнее и тренировать трансформер сложнее, хорошо выучить его не получилось.

Минимальное расстояние Левенштейна на тесте: 0.88

Возможно, трансформер не удалось хорошо выучить из-за моих ошибок. Возможно, в целом плохая идея тренировать трансформер на таких маленьких токенах, т.к. здесь будет tradeoff размерности эмбеддинга, т.к. Positional Encoding имеют такую же размерность, как и сами эмбеддинги, т.е. о позициях нам надо знать много, а о самих символах в них немного, и, кажется, что то из них либо сильно переобучается, либо недообучается.

# Дальнейшие шаги

- Регулярки и предобработка
- Все-таки попробовать обычную токенизацию и посмотреть, настолько ли все плохо из-за запятых и пробелов

