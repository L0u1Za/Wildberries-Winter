{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f812f4b5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-17T12:25:41.618344Z",
     "iopub.status.busy": "2025-01-17T12:25:41.618090Z",
     "iopub.status.idle": "2025-01-17T12:25:43.022126Z",
     "shell.execute_reply": "2025-01-17T12:25:43.021396Z"
    },
    "papermill": {
     "duration": 1.410677,
     "end_time": "2025-01-17T12:25:43.023748",
     "exception": false,
     "start_time": "2025-01-17T12:25:41.613071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key='b123af3ff1bc7e54569d0976c6405a5b3b6d2902')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904098ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T12:25:43.032986Z",
     "iopub.status.busy": "2025-01-17T12:25:43.032561Z",
     "iopub.status.idle": "2025-01-17T12:25:47.973594Z",
     "shell.execute_reply": "2025-01-17T12:25:47.972682Z"
    },
    "papermill": {
     "duration": 4.94767,
     "end_time": "2025-01-17T12:25:47.975617",
     "exception": false,
     "start_time": "2025-01-17T12:25:43.027947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f3d1e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T12:25:47.986113Z",
     "iopub.status.busy": "2025-01-17T12:25:47.985129Z",
     "iopub.status.idle": "2025-01-17T12:25:47.994816Z",
     "shell.execute_reply": "2025-01-17T12:25:47.994094Z"
    },
    "papermill": {
     "duration": 0.016206,
     "end_time": "2025-01-17T12:25:47.996396",
     "exception": false,
     "start_time": "2025-01-17T12:25:47.980190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text to retain only alphabetic characters and convert to lowercase.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): Input string.\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed string.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def all_preprocessing(df):\n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "def extract_words_from_mask(df, text_col='text', mask_col='label_new'):\n",
    "    \"\"\"\n",
    "    Extracts substrings from `text` based on the boolean mask in `label_new`.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe with columns `text` and `label_new`.\n",
    "        text_col (str): Name of the column containing the text.\n",
    "        mask_col (str): Name of the column containing the boolean mask.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: A Series where each entry is a list of words extracted from `text`.\n",
    "    \"\"\"\n",
    "    def process_row(row):\n",
    "        text = row[text_col]\n",
    "        mask = row[mask_col]\n",
    "        words = []\n",
    "        current_word = []\n",
    "        \n",
    "        for char, include in zip(text, mask):\n",
    "            if include:\n",
    "                current_word.append(char)\n",
    "            elif current_word:  # If a word is in progress and we hit a `0`\n",
    "                words.append(\"\".join(current_word))\n",
    "                current_word = []  # Reset for the next word\n",
    "        \n",
    "        if current_word:  # Append the last word if still in progress\n",
    "            words.append(\"\".join(current_word))\n",
    "        words = sorted(words)\n",
    "        return \",\".join(words)\n",
    "    \n",
    "    return df.apply(process_row, axis=1)\n",
    "\n",
    "\n",
    "def to_out_labels(texts, labels):\n",
    "    def process_row(i):\n",
    "        text = texts[i]\n",
    "        mask = labels[i]\n",
    "        words = []\n",
    "        current_word = []\n",
    "        \n",
    "        for char, include in zip(text, mask):\n",
    "            if include:\n",
    "                current_word.append(char)\n",
    "            elif current_word:  # If a word is in progress and we hit a `0`\n",
    "                words.append(\"\".join(current_word))\n",
    "                current_word = []  # Reset for the next word\n",
    "        \n",
    "        if current_word:  # Append the last word if still in progress\n",
    "            words.append(\"\".join(current_word))\n",
    "\n",
    "        words = sorted(words)\n",
    "        return \",\".join(words)\n",
    "    out_labels = [process_row(i) for i in range(len(texts))]\n",
    "    return out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc47ab71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T12:25:48.006127Z",
     "iopub.status.busy": "2025-01-17T12:25:48.005643Z",
     "iopub.status.idle": "2025-01-17T12:26:23.485025Z",
     "shell.execute_reply": "2025-01-17T12:26:23.484217Z"
    },
    "papermill": {
     "duration": 35.489828,
     "end_time": "2025-01-17T12:26:23.491036",
     "exception": false,
     "start_time": "2025-01-17T12:25:48.001208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248088</th>\n",
       "      <td>–º–Ω–µ –µ–µ –ø–æ—Ä–≤–∞–ª–∏ —Å—É–∫–∏</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248089</th>\n",
       "      <td>–ø–æ–ª–Ω–æ–µ –¥–µ—Ä—å–º–æ, —É–¥–∞–ª–∏—Ç–µ —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä –∏ –∑–∞–±–ª–æ–∫–∏—Ä—É–π...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248090</th>\n",
       "      <td>—Ö–µ—Ä–Ω—è. –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä.</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248091</th>\n",
       "      <td>—ç—Ç–æ –≤–æ–æ–±—â–µ —á—Ç–æ , –∑–∞ üí© –≥–æ–≤... —â–µ?? —Ç–µ–º–Ω–æ—Ç–∏—â–∞ —É–∂...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248092</th>\n",
       "      <td>–Ω–µ –±–µ—Ä–∏—Ç–µ!!!!! –º–µ–ª–∫–∏–µ, –ø–æ—Ä–µ–∑–∞–Ω–Ω—ã–µ, –ø–æ–¥–ø–æ—Ä—á–µ–Ω–Ω—ã...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "ID                                                          \n",
       "248088                                –º–Ω–µ –µ–µ –ø–æ—Ä–≤–∞–ª–∏ —Å—É–∫–∏   \n",
       "248089  –ø–æ–ª–Ω–æ–µ –¥–µ—Ä—å–º–æ, —É–¥–∞–ª–∏—Ç–µ —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä –∏ –∑–∞–±–ª–æ–∫–∏—Ä—É–π...   \n",
       "248090                            —Ö–µ—Ä–Ω—è. –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä.   \n",
       "248091  —ç—Ç–æ –≤–æ–æ–±—â–µ —á—Ç–æ , –∑–∞ üí© –≥–æ–≤... —â–µ?? —Ç–µ–º–Ω–æ—Ç–∏—â–∞ —É–∂...   \n",
       "248092  –Ω–µ –±–µ—Ä–∏—Ç–µ!!!!! –º–µ–ª–∫–∏–µ, –ø–æ—Ä–µ–∑–∞–Ω–Ω—ã–µ, –ø–æ–¥–ø–æ—Ä—á–µ–Ω–Ω—ã...   \n",
       "\n",
       "                                                    label  \n",
       "ID                                                         \n",
       "248088  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "248089  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, ...  \n",
       "248090  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "248091  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "248092  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/wb-contest-2/wb_contest_2_new_dataset.csv', index_col='ID')\n",
    "df['label'] = df['label'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f9fb390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T12:26:23.500377Z",
     "iopub.status.busy": "2025-01-17T12:26:23.500111Z",
     "iopub.status.idle": "2025-01-17T12:26:23.511587Z",
     "shell.execute_reply": "2025-01-17T12:26:23.510995Z"
    },
    "papermill": {
     "duration": 0.018045,
     "end_time": "2025-01-17T12:26:23.513112",
     "exception": false,
     "start_time": "2025-01-17T12:26:23.495067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom Tokenizer: Split into characters and map each character to an index\n",
    "class CharTokenizer:\n",
    "    def __init__(self, texts):\n",
    "        # Create a vocabulary from the unique characters in the dataset + a padding token (0)\n",
    "        self.vocab = {char: idx + 1 for idx, char in enumerate(set(''.join(texts)))}  # index 0 is for padding\n",
    "        self.vocab_size = len(self.vocab) + 1  # Add 1 for padding\n",
    "        self.vocab['<PAD>'] = 0  # Adding padding token\n",
    "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Return the indices of each character in the text based on the vocabulary\n",
    "        return [self.vocab.get(char, self.vocab['<PAD>']) for char in text]\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        # Decode token IDs back to characters\n",
    "        return ''.join([self.reverse_vocab.get(idx, '<PAD>') for idx in token_ids])\n",
    "\n",
    "# Custom Dataset\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=1024):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text into character-level tokens\n",
    "        input_ids = self.tokenizer.encode(text)\n",
    "        \n",
    "        # Pad the sequence to max_len if necessary\n",
    "        input_ids = input_ids + [0] * (self.max_len - len(input_ids)) if len(input_ids) < self.max_len else input_ids[:self.max_len]\n",
    "        \n",
    "        # Pad the labels to max_len if necessary\n",
    "        labels_padded = label + [0] * (self.max_len - len(label)) if len(label) < self.max_len else label[:self.max_len]\n",
    "        #print(sum(labels_padded))\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([1] * len(input_ids) + [0] * (self.max_len - len(input_ids)), dtype=torch.long),  # Attention mask\n",
    "            'labels': torch.tensor(labels_padded, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=1024):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(text)\n",
    "        input_ids = input_ids + [0] * (self.max_len - len(input_ids)) if len(input_ids) < self.max_len else input_ids[:self.max_len]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([1] * len(input_ids) + [0] * (self.max_len - len(input_ids)), dtype=torch.long)  # Attention mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b07e575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T12:26:23.522148Z",
     "iopub.status.busy": "2025-01-17T12:26:23.521910Z",
     "iopub.status.idle": "2025-01-17T12:26:23.528755Z",
     "shell.execute_reply": "2025-01-17T12:26:23.528107Z"
    },
    "papermill": {
     "duration": 0.013121,
     "end_time": "2025-01-17T12:26:23.530174",
     "exception": false,
     "start_time": "2025-01-17T12:26:23.517053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharTokenClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, transformer_dim=512, num_heads=8, num_layers=6, num_classes=2, dropout=0.2):\n",
    "        super(CharTokenClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Character embeddings\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 1024, embedding_dim))  # Learnable positional encoding\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=transformer_dim, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)  # Output classification layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get embeddings and add positional encoding\n",
    "        x = self.embedding(input_ids)  # (batch_size, seq_len, embedding_dim)\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        # Apply attention mask (if needed)\n",
    "        src_key_padding_mask = ~attention_mask.bool() if attention_mask is not None else None\n",
    "        \n",
    "        # Pass through the Transformer\n",
    "        x = self.transformer(x.transpose(0, 1), src_key_padding_mask=src_key_padding_mask)  # (seq_len, batch_size, embedding_dim)\n",
    "        x = x.transpose(0, 1)  # (batch_size, seq_len, embedding_dim)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply classifier on each token\n",
    "        logits = self.classifier(x)  # (batch_size, seq_len, num_classes)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d49153e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T12:26:23.539103Z",
     "iopub.status.busy": "2025-01-17T12:26:23.538867Z",
     "iopub.status.idle": "2025-01-17T12:26:24.694098Z",
     "shell.execute_reply": "2025-01-17T12:26:24.693237Z"
    },
    "papermill": {
     "duration": 1.161809,
     "end_time": "2025-01-17T12:26:24.695883",
     "exception": false,
     "start_time": "2025-01-17T12:26:23.534074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1131\n"
     ]
    }
   ],
   "source": [
    "texts = df['text'].values  # or df['text'].to_numpy()\n",
    "labels = df['label'].values  # or df['label'].to_numpy()\n",
    "\n",
    "tokenizer = CharTokenizer(texts)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = TokenClassificationDataset(texts, labels, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64e29d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T12:26:24.705315Z",
     "iopub.status.busy": "2025-01-17T12:26:24.705041Z",
     "iopub.status.idle": "2025-01-17T12:26:25.202686Z",
     "shell.execute_reply": "2025-01-17T12:26:25.202008Z"
    },
    "papermill": {
     "duration": 0.50439,
     "end_time": "2025-01-17T12:26:25.204554",
     "exception": false,
     "start_time": "2025-01-17T12:26:24.700164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_model(model, train_loader, epochs=10, lr=5e-5, warmup_steps=1000, max_grad_norm=1.0, device='cuda'):\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Learning rate scheduler with warmup\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    \n",
    "    # Mixed precision scaler\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "            input_ids, labels, attention_mask = batch[\"input_ids\"], batch[\"labels\"], batch[\"attention_mask\"]\n",
    "            input_ids, labels, attention_mask = input_ids.to(device), labels.to(device), attention_mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Mixed precision forward pass\n",
    "            with autocast():\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits.view(-1, 2), labels.view(-1))\n",
    "\n",
    "            # Backward pass with mixed precision\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            # Optimizer step with scaler\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Scheduler step\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f8d4a17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T12:26:25.214104Z",
     "iopub.status.busy": "2025-01-17T12:26:25.213694Z",
     "iopub.status.idle": "2025-01-17T20:21:33.782068Z",
     "shell.execute_reply": "2025-01-17T20:21:33.781244Z"
    },
    "papermill": {
     "duration": 28508.577213,
     "end_time": "2025-01-17T20:21:33.786036",
     "exception": false,
     "start_time": "2025-01-17T12:26:25.208823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/tmp/ipykernel_23/1728367869.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1/10:   0%|          | 0/3825 [00:00<?, ?batch/s]/tmp/ipykernel_23/1728367869.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [46:22<00:00,  1.37batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [47:53<00:00,  1.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [48:08<00:00,  1.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [48:09<00:00,  1.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [48:00<00:00,  1.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [48:04<00:00,  1.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [47:45<00:00,  1.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [46:44<00:00,  1.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [46:32<00:00,  1.37batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [47:26<00:00,  1.34batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CharTokenClassifier(vocab_size=tokenizer.vocab_size, embedding_dim=64)\n",
    "model.to(device)\n",
    "\n",
    "train_model(model, dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28107030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T20:21:37.541380Z",
     "iopub.status.busy": "2025-01-17T20:21:37.540812Z",
     "iopub.status.idle": "2025-01-17T20:21:37.558003Z",
     "shell.execute_reply": "2025-01-17T20:21:37.557394Z"
    },
    "papermill": {
     "duration": 1.877179,
     "end_time": "2025-01-17T20:21:37.559651",
     "exception": false,
     "start_time": "2025-01-17T20:21:35.682472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/kaggle/working/model.pth')\n",
    "\n",
    "import pickle\n",
    "with open('/kaggle/working/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "802d18b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T20:21:41.210169Z",
     "iopub.status.busy": "2025-01-17T20:21:41.209480Z",
     "iopub.status.idle": "2025-01-17T20:21:41.215688Z",
     "shell.execute_reply": "2025-01-17T20:21:41.214850Z"
    },
    "papermill": {
     "duration": 1.871904,
     "end_time": "2025-01-17T20:21:41.217193",
     "exception": false,
     "start_time": "2025-01-17T20:21:39.345289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndevice = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n\\n# Initialize the model, loss function, and optimizer\\nmodel = CharTokenClassifier(vocab_size=tokenizer.vocab_size, embedding_dim=64)\\nmodel.load_state_dict(torch.load(\\'/kaggle/working/model.pth\\', weights_only=True))\\nmodel.to(device)\\nmodel.eval()  # Set the model to evaluation mode\\n\\n# Function to run inference on the entire dataset or a sample\\ndef run_eval(dataloader):\\n    all_preds = []\\n    all_labels = []\\n\\n    with torch.no_grad():  # Disable gradient computation for inference\\n        for batch in tqdm(dataloader, desc=\"Running Inference\"):\\n            # Move batch data to the correct device\\n            input_ids = batch[\\'input_ids\\'].to(device)\\n            attention_mask = batch[\\'attention_mask\\'].to(device)\\n            labels = batch[\\'labels\\'].to(device)\\n\\n            # Run the model on the batch\\n            logits = model(input_ids, attention_mask)  # (batch_size, seq_len, num_classes)\\n\\n            # Apply softmax to get class probabilities\\n            probs = torch.softmax(logits, dim=-1)  # Shape: (batch_size, seq_len, num_classes)\\n\\n            # Get predicted class for each token\\n            predicted_class = torch.argmax(probs, dim=-1)  # Shape: (batch_size, seq_len)\\n\\n            # Store predictions and labels\\n            for i in range(len(labels)):\\n                all_preds.append(predicted_class[i].cpu().numpy())  # Move to CPU for easy handling\\n                all_labels.append(labels[i].cpu().numpy())\\n\\n    return all_preds, all_labels\\n\\n# Run inference\\n#predictions, ground_truth = run_eval(dataloader)\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CharTokenClassifier(vocab_size=tokenizer.vocab_size, embedding_dim=64)\n",
    "model.load_state_dict(torch.load('/kaggle/working/model.pth', weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to run inference on the entire dataset or a sample\n",
    "def run_eval(dataloader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for batch in tqdm(dataloader, desc=\"Running Inference\"):\n",
    "            # Move batch data to the correct device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Run the model on the batch\n",
    "            logits = model(input_ids, attention_mask)  # (batch_size, seq_len, num_classes)\n",
    "\n",
    "            # Apply softmax to get class probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # Shape: (batch_size, seq_len, num_classes)\n",
    "\n",
    "            # Get predicted class for each token\n",
    "            predicted_class = torch.argmax(probs, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            for i in range(len(labels)):\n",
    "                all_preds.append(predicted_class[i].cpu().numpy())  # Move to CPU for easy handling\n",
    "                all_labels.append(labels[i].cpu().numpy())\n",
    "\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Run inference\n",
    "#predictions, ground_truth = run_eval(dataloader)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdbcc38c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T20:21:44.888895Z",
     "iopub.status.busy": "2025-01-17T20:21:44.888567Z",
     "iopub.status.idle": "2025-01-17T20:21:44.893964Z",
     "shell.execute_reply": "2025-01-17T20:21:44.893169Z"
    },
    "papermill": {
     "duration": 1.875144,
     "end_time": "2025-01-17T20:21:44.895533",
     "exception": false,
     "start_time": "2025-01-17T20:21:43.020389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ni = 244736\\ntext = dataset.texts[i]\\n# Example: Print the first prediction and corresponding ground truth\\nprint(f\"Text: {text}\")\\nprint(f\"Prediction: {predictions[i]}\")\\nprint(f\"Ground truth: {ground_truth[i]}\")\\n\\npredicted = pd.DataFrame({\"text\": dataset.texts, \"prediction\": predictions, \"label\": ground_truth})\\npredicted[\\'label_out\\'] = extract_words_from_mask(predicted, mask_col=\\'prediction\\')\\n\\npredicted\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "i = 244736\n",
    "text = dataset.texts[i]\n",
    "# Example: Print the first prediction and corresponding ground truth\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Prediction: {predictions[i]}\")\n",
    "print(f\"Ground truth: {ground_truth[i]}\")\n",
    "\n",
    "predicted = pd.DataFrame({\"text\": dataset.texts, \"prediction\": predictions, \"label\": ground_truth})\n",
    "predicted['label_out'] = extract_words_from_mask(predicted, mask_col='prediction')\n",
    "\n",
    "predicted\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "005151d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T20:21:48.530965Z",
     "iopub.status.busy": "2025-01-17T20:21:48.530182Z",
     "iopub.status.idle": "2025-01-17T20:21:48.533991Z",
     "shell.execute_reply": "2025-01-17T20:21:48.533298Z"
    },
    "papermill": {
     "duration": 1.857501,
     "end_time": "2025-01-17T20:21:48.535502",
     "exception": false,
     "start_time": "2025-01-17T20:21:46.678001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predicted.to_csv(\"/kaggle/working/predicted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43aea1ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T20:21:52.238145Z",
     "iopub.status.busy": "2025-01-17T20:21:52.237827Z",
     "iopub.status.idle": "2025-01-17T20:21:52.243432Z",
     "shell.execute_reply": "2025-01-17T20:21:52.242558Z"
    },
    "papermill": {
     "duration": 1.920356,
     "end_time": "2025-01-17T20:21:52.245000",
     "exception": false,
     "start_time": "2025-01-17T20:21:50.324644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\\n# Initialize the model, loss function, and optimizer\\nmodel = CharTokenClassifier(vocab_size=tokenizer.vocab_size, embedding_dim=64)\\nmodel.load_state_dict(torch.load('/kaggle/working/model.pth', weights_only=True))\\nmodel.to(device)\\nmodel.eval()\\n\\nimport pickle\\nwith open('/kaggle/input/working/tokenizer.pickle', 'rb') as handle:\\n    tokenizer = pickle.load(handle)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CharTokenClassifier(vocab_size=tokenizer.vocab_size, embedding_dim=64)\n",
    "model.load_state_dict(torch.load('/kaggle/working/model.pth', weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "import pickle\n",
    "with open('/kaggle/input/working/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37c15dfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T20:21:55.928013Z",
     "iopub.status.busy": "2025-01-17T20:21:55.927688Z",
     "iopub.status.idle": "2025-01-17T20:21:56.945757Z",
     "shell.execute_reply": "2025-01-17T20:21:56.945055Z"
    },
    "papermill": {
     "duration": 2.881314,
     "end_time": "2025-01-17T20:21:56.947712",
     "exception": false,
     "start_time": "2025-01-17T20:21:54.066398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "df_test = pd.read_csv('/kaggle/input/wildberries-winter-school-24-contest-detected/test.csv')\n",
    "df_test = all_preprocessing(df_test)\n",
    "\n",
    "texts = df_test['text'].values  # or df['text'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "672e44a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T20:22:00.634483Z",
     "iopub.status.busy": "2025-01-17T20:22:00.634110Z",
     "iopub.status.idle": "2025-01-17T20:25:54.660003Z",
     "shell.execute_reply": "2025-01-17T20:25:54.659217Z"
    },
    "papermill": {
     "duration": 235.91388,
     "end_time": "2025-01-17T20:25:54.662030",
     "exception": false,
     "start_time": "2025-01-17T20:21:58.748150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1047/1047 [03:51<00:00,  4.53it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_test = InferenceDataset(texts, tokenizer)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=False)\n",
    "\n",
    "def run_inference(model, dataloader, device):\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for batch in tqdm(dataloader, desc=\"Running Inference\"):\n",
    "            # Move batch data to the correct device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Run the model on the batch\n",
    "            logits = model(input_ids, attention_mask)  # (batch_size, seq_len, num_classes)\n",
    "\n",
    "            # Apply softmax to get class probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # Shape: (batch_size, seq_len, num_classes)\n",
    "\n",
    "            # Get predicted class for each token\n",
    "            predicted_class = torch.argmax(probs, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            for i in range(len(predicted_class)):\n",
    "                all_preds.append(predicted_class[i].cpu().numpy())  # Move to CPU for easy handling\n",
    "\n",
    "    return all_preds\n",
    "\n",
    "# Run inference\n",
    "predictions = run_inference(model, dataloader_test, device)\n",
    "\n",
    "df_test[\"label_new\"] = predictions\n",
    "df_test.to_csv('/kaggle/working/out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0413ec1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T20:25:58.461534Z",
     "iopub.status.busy": "2025-01-17T20:25:58.460795Z",
     "iopub.status.idle": "2025-01-17T20:26:02.741521Z",
     "shell.execute_reply": "2025-01-17T20:26:02.740575Z"
    },
    "papermill": {
     "duration": 6.146866,
     "end_time": "2025-01-17T20:26:02.743154",
     "exception": false,
     "start_time": "2025-01-17T20:25:56.596288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label_new</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>—Ö–æ—Ä–æ—à–∏–π, –ø–æ–¥–æ—à–∫–ª</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>—Å–æ–≤—Å–µ–º —Ç–æ–Ω—é—Å–µ–Ω—å–∫–∏–π —Å–∞–∂–µ–Ω–µ—Ü, –Ω–µ –¥–æ—Å–º–æ—Ç—Ä–µ–ª–∞ –≤ –æ–ø...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>–∫–æ–≥—Ç–µ—Ç–æ—á–∫–∞ —Ö–æ—Ä–æ—à–∞—è, –Ω–æ –≤–æ—Ç —Ç–∫–∞–Ω—å –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>–º–Ω–æ–≥–æ –∑–∞—Ç—è–∂–µ–∫, –Ω–µ –ø–æ—Ä–∞–¥–æ–≤–∞–ª–∞ –ø–æ–∫—É–ø–∫–∞</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>—Ä–µ–∫–æ–º–µ–Ω–¥—É—é üí£</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66944</th>\n",
       "      <td>71995</td>\n",
       "      <td>–ø–∞—á–∫–∞ –∫–∞–∫ –ø–∞—á–∫–∞ –∞ –≤–Ω—É—Ç—Ä–∏ —Å–æ–≤—Å–µ–º –¥—Ä—É–≥–æ–µ –Ω–µ–∫—Ç–æ—Ä–≤...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66945</th>\n",
       "      <td>71996</td>\n",
       "      <td>–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ!!! —á–µ—Ä–µ–∑ –≥–æ–¥ —Ç—Ä—è–ø–∫–∞, —É...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66946</th>\n",
       "      <td>71997</td>\n",
       "      <td>–≤–µ—Å 100–≥—Ä, –Ω–µ –ø–æ–Ω–∏–º–∞—é –æ—Ç–∫—É–¥–∞ —Ö–æ—Ä–æ—à–∏–µ –æ—Ç–∑—ã–≤—ã , ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66947</th>\n",
       "      <td>71998</td>\n",
       "      <td>–ø—Ä–∏–æ–±—Ä–µ–ª –∏ —É—Å—Ç–∞–Ω–æ–≤–∏–ª —Ä–∞–¥–∏–∞—Ç–æ—Ä –≥–æ–¥ –Ω–∞–∑–∞–¥ , –∞–≤—Ç–æ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66948</th>\n",
       "      <td>71999</td>\n",
       "      <td>–∑–∞–µ–±–∏—Å—å —Ä—É—á–∫–∞, –ø—Ä–∞–≤–¥–∞ –Ω–µ –ø–∏—à–µ—Ç(</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66949 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                               text  \\\n",
       "0          0                                   —Ö–æ—Ä–æ—à–∏–π, –ø–æ–¥–æ—à–∫–ª   \n",
       "1          1  —Å–æ–≤—Å–µ–º —Ç–æ–Ω—é—Å–µ–Ω—å–∫–∏–π —Å–∞–∂–µ–Ω–µ—Ü, –Ω–µ –¥–æ—Å–º–æ—Ç—Ä–µ–ª–∞ –≤ –æ–ø...   \n",
       "2          2  –∫–æ–≥—Ç–µ—Ç–æ—á–∫–∞ —Ö–æ—Ä–æ—à–∞—è, –Ω–æ –≤–æ—Ç —Ç–∫–∞–Ω—å –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ ...   \n",
       "3          3               –º–Ω–æ–≥–æ –∑–∞—Ç—è–∂–µ–∫, –Ω–µ –ø–æ—Ä–∞–¥–æ–≤–∞–ª–∞ –ø–æ–∫—É–ø–∫–∞   \n",
       "4          4                                       —Ä–µ–∫–æ–º–µ–Ω–¥—É—é üí£   \n",
       "...      ...                                                ...   \n",
       "66944  71995  –ø–∞—á–∫–∞ –∫–∞–∫ –ø–∞—á–∫–∞ –∞ –≤–Ω—É—Ç—Ä–∏ —Å–æ–≤—Å–µ–º –¥—Ä—É–≥–æ–µ –Ω–µ–∫—Ç–æ—Ä–≤...   \n",
       "66945  71996  –æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ!!! —á–µ—Ä–µ–∑ –≥–æ–¥ —Ç—Ä—è–ø–∫–∞, —É...   \n",
       "66946  71997  –≤–µ—Å 100–≥—Ä, –Ω–µ –ø–æ–Ω–∏–º–∞—é –æ—Ç–∫—É–¥–∞ —Ö–æ—Ä–æ—à–∏–µ –æ—Ç–∑—ã–≤—ã , ...   \n",
       "66947  71998  –ø—Ä–∏–æ–±—Ä–µ–ª –∏ —É—Å—Ç–∞–Ω–æ–≤–∏–ª —Ä–∞–¥–∏–∞—Ç–æ—Ä –≥–æ–¥ –Ω–∞–∑–∞–¥ , –∞–≤—Ç–æ...   \n",
       "66948  71999                    –∑–∞–µ–±–∏—Å—å —Ä—É—á–∫–∞, –ø—Ä–∞–≤–¥–∞ –Ω–µ –ø–∏—à–µ—Ç(   \n",
       "\n",
       "                                               label_new label  \n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        \n",
       "...                                                  ...   ...  \n",
       "66944  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        \n",
       "66945  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        \n",
       "66946  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        \n",
       "66947  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        \n",
       "66948  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        \n",
       "\n",
       "[66949 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'] = extract_words_from_mask(df_test)\n",
    "\n",
    "df_test.to_csv('/kaggle/working/out_with_words.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a9a6588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T20:26:06.574135Z",
     "iopub.status.busy": "2025-01-17T20:26:06.573337Z",
     "iopub.status.idle": "2025-01-17T20:26:06.617211Z",
     "shell.execute_reply": "2025-01-17T20:26:06.616597Z"
    },
    "papermill": {
     "duration": 1.909904,
     "end_time": "2025-01-17T20:26:06.618974",
     "exception": false,
     "start_time": "2025-01-17T20:26:04.709070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.drop(columns=['label_new', 'text']).set_index(\"ID\").to_csv(\"/kaggle/working/final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e80af1",
   "metadata": {
    "papermill": {
     "duration": 1.866184,
     "end_time": "2025-01-17T20:26:10.430170",
     "exception": false,
     "start_time": "2025-01-17T20:26:08.563986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10312867,
     "sourceId": 88517,
     "sourceType": "competition"
    },
    {
     "datasetId": 6200792,
     "sourceId": 10061944,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6228410,
     "sourceId": 10098814,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28836.259938,
   "end_time": "2025-01-17T20:26:15.432675",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-17T12:25:39.172737",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
