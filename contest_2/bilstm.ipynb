{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63486303",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-03T14:12:31.538546Z",
     "iopub.status.busy": "2024-12-03T14:12:31.538292Z",
     "iopub.status.idle": "2024-12-03T14:12:33.476849Z",
     "shell.execute_reply": "2024-12-03T14:12:33.475989Z"
    },
    "papermill": {
     "duration": 1.945427,
     "end_time": "2024-12-03T14:12:33.478423",
     "exception": false,
     "start_time": "2024-12-03T14:12:31.532996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key='b123af3ff1bc7e54569d0976c6405a5b3b6d2902')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3244a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:12:33.487139Z",
     "iopub.status.busy": "2024-12-03T14:12:33.486878Z",
     "iopub.status.idle": "2024-12-03T14:12:38.443316Z",
     "shell.execute_reply": "2024-12-03T14:12:38.442482Z"
    },
    "papermill": {
     "duration": 4.963466,
     "end_time": "2024-12-03T14:12:38.445848",
     "exception": false,
     "start_time": "2024-12-03T14:12:33.482382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b461cde2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:12:38.455341Z",
     "iopub.status.busy": "2024-12-03T14:12:38.454915Z",
     "iopub.status.idle": "2024-12-03T14:12:38.464272Z",
     "shell.execute_reply": "2024-12-03T14:12:38.463456Z"
    },
    "papermill": {
     "duration": 0.015964,
     "end_time": "2024-12-03T14:12:38.465891",
     "exception": false,
     "start_time": "2024-12-03T14:12:38.449927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text to retain only alphabetic characters and convert to lowercase.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): Input string.\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed string.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def all_preprocessing(df):\n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "def extract_words_from_mask(df, text_col='text', mask_col='label_new'):\n",
    "    \"\"\"\n",
    "    Extracts substrings from `text` based on the boolean mask in `label_new`.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe with columns `text` and `label_new`.\n",
    "        text_col (str): Name of the column containing the text.\n",
    "        mask_col (str): Name of the column containing the boolean mask.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: A Series where each entry is a list of words extracted from `text`.\n",
    "    \"\"\"\n",
    "    def process_row(row):\n",
    "        text = row[text_col]\n",
    "        mask = row[mask_col]\n",
    "        words = []\n",
    "        current_word = []\n",
    "        \n",
    "        for char, include in zip(text, mask):\n",
    "            if include:\n",
    "                current_word.append(char)\n",
    "            elif current_word:  # If a word is in progress and we hit a `0`\n",
    "                words.append(\"\".join(current_word))\n",
    "                current_word = []  # Reset for the next word\n",
    "        \n",
    "        if current_word:  # Append the last word if still in progress\n",
    "            words.append(\"\".join(current_word))\n",
    "        words = sorted(words)\n",
    "        return \",\".join(words)\n",
    "    \n",
    "    return df.apply(process_row, axis=1)\n",
    "\n",
    "\n",
    "def to_out_labels(texts, labels):\n",
    "    def process_row(i):\n",
    "        text = texts[i]\n",
    "        mask = labels[i]\n",
    "        words = []\n",
    "        current_word = []\n",
    "        \n",
    "        for char, include in zip(text, mask):\n",
    "            if include:\n",
    "                current_word.append(char)\n",
    "            elif current_word:  # If a word is in progress and we hit a `0`\n",
    "                words.append(\"\".join(current_word))\n",
    "                current_word = []  # Reset for the next word\n",
    "        \n",
    "        if current_word:  # Append the last word if still in progress\n",
    "            words.append(\"\".join(current_word))\n",
    "\n",
    "        words = sorted(words)\n",
    "        return \",\".join(words)\n",
    "    out_labels = [process_row(i) for i in range(len(texts))]\n",
    "    return out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aab224b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:12:38.475153Z",
     "iopub.status.busy": "2024-12-03T14:12:38.474902Z",
     "iopub.status.idle": "2024-12-03T14:13:14.119719Z",
     "shell.execute_reply": "2024-12-03T14:13:14.118875Z"
    },
    "papermill": {
     "duration": 35.655798,
     "end_time": "2024-12-03T14:13:14.125270",
     "exception": false,
     "start_time": "2024-12-03T14:12:38.469472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248088</th>\n",
       "      <td>–º–Ω–µ –µ–µ –ø–æ—Ä–≤–∞–ª–∏ —Å—É–∫–∏</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248089</th>\n",
       "      <td>–ø–æ–ª–Ω–æ–µ –¥–µ—Ä—å–º–æ, —É–¥–∞–ª–∏—Ç–µ —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä –∏ –∑–∞–±–ª–æ–∫–∏—Ä—É–π...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248090</th>\n",
       "      <td>—Ö–µ—Ä–Ω—è. –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä.</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248091</th>\n",
       "      <td>—ç—Ç–æ –≤–æ–æ–±—â–µ —á—Ç–æ , –∑–∞ üí© –≥–æ–≤... —â–µ?? —Ç–µ–º–Ω–æ—Ç–∏—â–∞ —É–∂...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248092</th>\n",
       "      <td>–Ω–µ –±–µ—Ä–∏—Ç–µ!!!!! –º–µ–ª–∫–∏–µ, –ø–æ—Ä–µ–∑–∞–Ω–Ω—ã–µ, –ø–æ–¥–ø–æ—Ä—á–µ–Ω–Ω—ã...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "ID                                                          \n",
       "248088                                –º–Ω–µ –µ–µ –ø–æ—Ä–≤–∞–ª–∏ —Å—É–∫–∏   \n",
       "248089  –ø–æ–ª–Ω–æ–µ –¥–µ—Ä—å–º–æ, —É–¥–∞–ª–∏—Ç–µ —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä –∏ –∑–∞–±–ª–æ–∫–∏—Ä—É–π...   \n",
       "248090                            —Ö–µ—Ä–Ω—è. –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä.   \n",
       "248091  —ç—Ç–æ –≤–æ–æ–±—â–µ —á—Ç–æ , –∑–∞ üí© –≥–æ–≤... —â–µ?? —Ç–µ–º–Ω–æ—Ç–∏—â–∞ —É–∂...   \n",
       "248092  –Ω–µ –±–µ—Ä–∏—Ç–µ!!!!! –º–µ–ª–∫–∏–µ, –ø–æ—Ä–µ–∑–∞–Ω–Ω—ã–µ, –ø–æ–¥–ø–æ—Ä—á–µ–Ω–Ω—ã...   \n",
       "\n",
       "                                                    label  \n",
       "ID                                                         \n",
       "248088  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "248089  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, ...  \n",
       "248090  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "248091  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "248092  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/wb-contest-2/wb_contest_2_new_dataset.csv', index_col='ID')\n",
    "df['label'] = df['label'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222679a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:13:14.134070Z",
     "iopub.status.busy": "2024-12-03T14:13:14.133814Z",
     "iopub.status.idle": "2024-12-03T14:13:14.144767Z",
     "shell.execute_reply": "2024-12-03T14:13:14.144104Z"
    },
    "papermill": {
     "duration": 0.01729,
     "end_time": "2024-12-03T14:13:14.146367",
     "exception": false,
     "start_time": "2024-12-03T14:13:14.129077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom Tokenizer: Split into characters and map each character to an index\n",
    "class CharTokenizer:\n",
    "    def __init__(self, texts):\n",
    "        # Create a vocabulary from the unique characters in the dataset + a padding token (0)\n",
    "        self.vocab = {char: idx + 1 for idx, char in enumerate(set(''.join(texts)))}  # index 0 is for padding\n",
    "        self.vocab_size = len(self.vocab) + 1  # Add 1 for padding\n",
    "        self.vocab['<PAD>'] = 0  # Adding padding token\n",
    "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Return the indices of each character in the text based on the vocabulary\n",
    "        return [self.vocab.get(char, self.vocab['<PAD>']) for char in text]\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        # Decode token IDs back to characters\n",
    "        return ''.join([self.reverse_vocab.get(idx, '<PAD>') for idx in token_ids])\n",
    "\n",
    "# Custom Dataset\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=1024):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text into character-level tokens\n",
    "        input_ids = self.tokenizer.encode(text)\n",
    "        \n",
    "        # Pad the sequence to max_len if necessary\n",
    "        input_ids = input_ids + [0] * (self.max_len - len(input_ids)) if len(input_ids) < self.max_len else input_ids[:self.max_len]\n",
    "        \n",
    "        # Pad the labels to max_len if necessary\n",
    "        labels_padded = label + [0] * (self.max_len - len(label)) if len(label) < self.max_len else label[:self.max_len]\n",
    "        #print(sum(labels_padded))\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([1] * len(input_ids) + [0] * (self.max_len - len(input_ids)), dtype=torch.long),  # Attention mask\n",
    "            'labels': torch.tensor(labels_padded, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=1024):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(text)\n",
    "        input_ids = input_ids + [0] * (self.max_len - len(input_ids)) if len(input_ids) < self.max_len else input_ids[:self.max_len]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([1] * len(input_ids) + [0] * (self.max_len - len(input_ids)), dtype=torch.long)  # Attention mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "527520e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:13:14.155011Z",
     "iopub.status.busy": "2024-12-03T14:13:14.154783Z",
     "iopub.status.idle": "2024-12-03T14:13:14.160506Z",
     "shell.execute_reply": "2024-12-03T14:13:14.159706Z"
    },
    "papermill": {
     "duration": 0.011894,
     "end_time": "2024-12-03T14:13:14.162130",
     "exception": false,
     "start_time": "2024-12-03T14:13:14.150236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model for token classification (character-level)\n",
    "class CharTokenClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_dim=128, num_classes=2, num_layers=2, bidirectional=True, dropout=0.3):\n",
    "        super(CharTokenClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Character embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(lstm_dim * 2 if bidirectional else lstm_dim, num_classes)  # Adjusted for bidirectional\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get embeddings for each character token\n",
    "        x = self.embedding(input_ids)  # (batch_size, seq_len, embedding_dim)\n",
    "        x, (hn, cn) = self.lstm(x)  # (batch_size, seq_len, hidden_dim)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)  # (batch_size, seq_len, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802c248c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:13:14.170901Z",
     "iopub.status.busy": "2024-12-03T14:13:14.170340Z",
     "iopub.status.idle": "2024-12-03T14:13:15.329808Z",
     "shell.execute_reply": "2024-12-03T14:13:15.328968Z"
    },
    "papermill": {
     "duration": 1.165458,
     "end_time": "2024-12-03T14:13:15.331353",
     "exception": false,
     "start_time": "2024-12-03T14:13:14.165895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1131\n"
     ]
    }
   ],
   "source": [
    "texts = df['text'].values  # or df['text'].to_numpy()\n",
    "labels = df['label'].values  # or df['label'].to_numpy()\n",
    "\n",
    "tokenizer = CharTokenizer(texts)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = TokenClassificationDataset(texts, labels, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241f926f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:13:15.340465Z",
     "iopub.status.busy": "2024-12-03T14:13:15.340184Z",
     "iopub.status.idle": "2024-12-03T14:47:12.669553Z",
     "shell.execute_reply": "2024-12-03T14:47:12.668493Z"
    },
    "papermill": {
     "duration": 2037.336377,
     "end_time": "2024-12-03T14:47:12.671795",
     "exception": false,
     "start_time": "2024-12-03T14:13:15.335418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 957/957 [06:47<00:00,  2.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 957/957 [06:47<00:00,  2.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 957/957 [06:47<00:00,  2.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 957/957 [06:47<00:00,  2.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 957/957 [06:46<00:00,  2.35batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CharTokenClassifier(vocab_size=tokenizer.vocab_size, embedding_dim=32)\n",
    "loss_fn = nn.CrossEntropyLoss()  # For token classification (binary)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "        # Get inputs and labels\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits.view(-1, 2), labels.view(-1))  # Flatten the logits and labels\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90004caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:47:13.089039Z",
     "iopub.status.busy": "2024-12-03T14:47:13.088471Z",
     "iopub.status.idle": "2024-12-03T14:47:13.101080Z",
     "shell.execute_reply": "2024-12-03T14:47:13.100240Z"
    },
    "papermill": {
     "duration": 0.221244,
     "end_time": "2024-12-03T14:47:13.102821",
     "exception": false,
     "start_time": "2024-12-03T14:47:12.881577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/kaggle/working/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5c13888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:47:13.527919Z",
     "iopub.status.busy": "2024-12-03T14:47:13.527532Z",
     "iopub.status.idle": "2024-12-03T14:51:07.477008Z",
     "shell.execute_reply": "2024-12-03T14:51:07.476061Z"
    },
    "papermill": {
     "duration": 234.160626,
     "end_time": "2024-12-03T14:51:07.479157",
     "exception": false,
     "start_time": "2024-12-03T14:47:13.318531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 957/957 [03:53<00:00,  4.09it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CharTokenClassifier(vocab_size=tokenizer.vocab_size, embedding_dim=32)\n",
    "model.load_state_dict(torch.load('/kaggle/working/model.pth', weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to run inference on the entire dataset or a sample\n",
    "def run_eval(dataloader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for batch in tqdm(dataloader, desc=\"Running Inference\"):\n",
    "            # Move batch data to the correct device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Run the model on the batch\n",
    "            logits = model(input_ids, attention_mask)  # (batch_size, seq_len, num_classes)\n",
    "\n",
    "            # Apply softmax to get class probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # Shape: (batch_size, seq_len, num_classes)\n",
    "\n",
    "            # Get predicted class for each token\n",
    "            predicted_class = torch.argmax(probs, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            for i in range(len(labels)):\n",
    "                all_preds.append(predicted_class[i].cpu().numpy())  # Move to CPU for easy handling\n",
    "                all_labels.append(labels[i].cpu().numpy())\n",
    "\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Run inference\n",
    "predictions, ground_truth = run_eval(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d87b119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:51:07.991855Z",
     "iopub.status.busy": "2024-12-03T14:51:07.991495Z",
     "iopub.status.idle": "2024-12-03T14:51:12.343616Z",
     "shell.execute_reply": "2024-12-03T14:51:12.342739Z"
    },
    "papermill": {
     "duration": 4.604907,
     "end_time": "2024-12-03T14:51:12.345558",
     "exception": false,
     "start_time": "2024-12-03T14:51:07.740651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: —Ö–µ—Ä–Ω—è. –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä.\n",
      "Prediction: [0 0 0 ... 0 0 0]\n",
      "Ground truth: [0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prediction</th>\n",
       "      <th>label</th>\n",
       "      <th>label_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–±—Ä—é–∫–∏ –æ—Ç–ª–∏—á–Ω—ã–µ, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ, –Ω–æ –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –∫...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>–±—Ä—é–∫–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–æ—Ç–ª–∏—á–Ω—ã–π –∞–ø–ø–∞—Ä–∞—Ç, –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ –∫–∞–±–µ–ª—å –∏ —Ä–∞–±–æ—Ç–µ—Ç...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Å—É–ø–µ—Ä üëç —Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–ø–æ–ª—É—á–∏–ª –±—ã—Å—Ç—Ä–æ –¥–∞–Ω–Ω—ã–µ –Ω–∞—É—à–Ω–∏–∫–∏! –∫ –Ω–∞—É—à–Ω–∏–∫–∞–º –ø—Ä...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤—Å—ë –¥–æ—à–ª–æ –≤ —Ü–µ–ª–æ—Å—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–Ω–æ—Å—Ç–∏)</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244734</th>\n",
       "      <td>–º–Ω–µ –µ–µ –ø–æ—Ä–≤–∞–ª–∏ —Å—É–∫–∏</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244735</th>\n",
       "      <td>–ø–æ–ª–Ω–æ–µ –¥–µ—Ä—å–º–æ, —É–¥–∞–ª–∏—Ç–µ —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä –∏ –∑–∞–±–ª–æ–∫–∏—Ä—É–π...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244736</th>\n",
       "      <td>—Ö–µ—Ä–Ω—è. –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä.</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244737</th>\n",
       "      <td>—ç—Ç–æ –≤–æ–æ–±—â–µ —á—Ç–æ , –∑–∞ üí© –≥–æ–≤... —â–µ?? —Ç–µ–º–Ω–æ—Ç–∏—â–∞ —É–∂...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244738</th>\n",
       "      <td>–Ω–µ –±–µ—Ä–∏—Ç–µ!!!!! –º–µ–ª–∫–∏–µ, –ø–æ—Ä–µ–∑–∞–Ω–Ω—ã–µ, –ø–æ–¥–ø–æ—Ä—á–µ–Ω–Ω—ã...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244739 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       –±—Ä—é–∫–∏ –æ—Ç–ª–∏—á–Ω—ã–µ, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ, –Ω–æ –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –∫...   \n",
       "1       –æ—Ç–ª–∏—á–Ω—ã–π –∞–ø–ø–∞—Ä–∞—Ç, –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ –∫–∞–±–µ–ª—å –∏ —Ä–∞–±–æ—Ç–µ—Ç...   \n",
       "2                                 —Å—É–ø–µ—Ä üëç —Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ   \n",
       "3       –ø–æ–ª—É—á–∏–ª –±—ã—Å—Ç—Ä–æ –¥–∞–Ω–Ω—ã–µ –Ω–∞—É—à–Ω–∏–∫–∏! –∫ –Ω–∞—É—à–Ω–∏–∫–∞–º –ø—Ä...   \n",
       "4                      –≤—Å—ë –¥–æ—à–ª–æ –≤ —Ü–µ–ª–æ—Å—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–Ω–æ—Å—Ç–∏)   \n",
       "...                                                   ...   \n",
       "244734                                –º–Ω–µ –µ–µ –ø–æ—Ä–≤–∞–ª–∏ —Å—É–∫–∏   \n",
       "244735  –ø–æ–ª–Ω–æ–µ –¥–µ—Ä—å–º–æ, —É–¥–∞–ª–∏—Ç–µ —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä –∏ –∑–∞–±–ª–æ–∫–∏—Ä—É–π...   \n",
       "244736                            —Ö–µ—Ä–Ω—è. –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä.   \n",
       "244737  —ç—Ç–æ –≤–æ–æ–±—â–µ —á—Ç–æ , –∑–∞ üí© –≥–æ–≤... —â–µ?? —Ç–µ–º–Ω–æ—Ç–∏—â–∞ —É–∂...   \n",
       "244738  –Ω–µ –±–µ—Ä–∏—Ç–µ!!!!! –º–µ–ª–∫–∏–µ, –ø–æ—Ä–µ–∑–∞–Ω–Ω—ã–µ, –ø–æ–¥–ø–æ—Ä—á–µ–Ω–Ω—ã...   \n",
       "\n",
       "                                               prediction  \\\n",
       "0       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                   ...   \n",
       "244734  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "244735  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "244736  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "244737  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "244738  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                    label label_out  \n",
       "0       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     –±—Ä—é–∫–∏  \n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            \n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            \n",
       "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            \n",
       "...                                                   ...       ...  \n",
       "244734  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            \n",
       "244735  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            \n",
       "244736  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            \n",
       "244737  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            \n",
       "244738  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            \n",
       "\n",
       "[244739 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 244736\n",
    "text = dataset.texts[i]\n",
    "# Example: Print the first prediction and corresponding ground truth\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Prediction: {predictions[i]}\")\n",
    "print(f\"Ground truth: {ground_truth[i]}\")\n",
    "\n",
    "predicted = pd.DataFrame({\"text\": dataset.texts, \"prediction\": predictions, \"label\": ground_truth})\n",
    "predicted['label_out'] = extract_words_from_mask(predicted, mask_col='prediction')\n",
    "\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc98c371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:51:12.893064Z",
     "iopub.status.busy": "2024-12-03T14:51:12.892314Z",
     "iopub.status.idle": "2024-12-03T14:51:32.893747Z",
     "shell.execute_reply": "2024-12-03T14:51:32.893021Z"
    },
    "papermill": {
     "duration": 20.294095,
     "end_time": "2024-12-03T14:51:32.895961",
     "exception": false,
     "start_time": "2024-12-03T14:51:12.601866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted.to_csv(\"/kaggle/working/predicted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2dccb50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:51:33.399296Z",
     "iopub.status.busy": "2024-12-03T14:51:33.398720Z",
     "iopub.status.idle": "2024-12-03T14:51:33.419707Z",
     "shell.execute_reply": "2024-12-03T14:51:33.418829Z"
    },
    "papermill": {
     "duration": 0.272375,
     "end_time": "2024-12-03T14:51:33.421242",
     "exception": false,
     "start_time": "2024-12-03T14:51:33.148867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharTokenClassifier(\n",
       "  (embedding): Embedding(1131, 32)\n",
       "  (lstm): LSTM(32, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CharTokenClassifier(vocab_size=tokenizer.vocab_size, embedding_dim=32)\n",
    "model.load_state_dict(torch.load('/kaggle/working/model.pth', weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6f06645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:51:33.923621Z",
     "iopub.status.busy": "2024-12-03T14:51:33.923067Z",
     "iopub.status.idle": "2024-12-03T14:51:34.916380Z",
     "shell.execute_reply": "2024-12-03T14:51:34.915710Z"
    },
    "papermill": {
     "duration": 1.244271,
     "end_time": "2024-12-03T14:51:34.918265",
     "exception": false,
     "start_time": "2024-12-03T14:51:33.673994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/kaggle/input/wildberries-winter-school-24-contest-detected/test.csv')\n",
    "df_test = all_preprocessing(df_test)\n",
    "\n",
    "texts = df_test['text'].values  # or df['text'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33eab0e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:51:35.458656Z",
     "iopub.status.busy": "2024-12-03T14:51:35.457796Z",
     "iopub.status.idle": "2024-12-03T14:52:33.012663Z",
     "shell.execute_reply": "2024-12-03T14:52:33.011962Z"
    },
    "papermill": {
     "duration": 57.807758,
     "end_time": "2024-12-03T14:52:33.014579",
     "exception": false,
     "start_time": "2024-12-03T14:51:35.206821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 262/262 [00:54<00:00,  4.80it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_test = InferenceDataset(texts, tokenizer)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256, shuffle=False)\n",
    "\n",
    "def run_inference(model, dataloader, device):\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for batch in tqdm(dataloader, desc=\"Running Inference\"):\n",
    "            # Move batch data to the correct device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Run the model on the batch\n",
    "            logits = model(input_ids, attention_mask)  # (batch_size, seq_len, num_classes)\n",
    "\n",
    "            # Apply softmax to get class probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # Shape: (batch_size, seq_len, num_classes)\n",
    "\n",
    "            # Get predicted class for each token\n",
    "            predicted_class = torch.argmax(probs, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            for i in range(len(predicted_class)):\n",
    "                all_preds.append(predicted_class[i].cpu().numpy())  # Move to CPU for easy handling\n",
    "\n",
    "    return all_preds\n",
    "\n",
    "# Run inference\n",
    "predictions = run_inference(model, dataloader_test, device)\n",
    "\n",
    "df_test[\"label_new\"] = predictions\n",
    "df_test.to_csv('/kaggle/working/out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6bd69c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:52:33.580161Z",
     "iopub.status.busy": "2024-12-03T14:52:33.579821Z",
     "iopub.status.idle": "2024-12-03T14:52:37.706801Z",
     "shell.execute_reply": "2024-12-03T14:52:37.705895Z"
    },
    "papermill": {
     "duration": 4.430209,
     "end_time": "2024-12-03T14:52:37.708393",
     "exception": false,
     "start_time": "2024-12-03T14:52:33.278184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label_new</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>—Ö–æ—Ä–æ—à–∏–π, –ø–æ–¥–æ—à–∫–ª</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>—Å–æ–≤—Å–µ–º —Ç–æ–Ω—é—Å–µ–Ω—å–∫–∏–π —Å–∞–∂–µ–Ω–µ—Ü, –Ω–µ –¥–æ—Å–º–æ—Ç—Ä–µ–ª–∞ –≤ –æ–ø...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>–∫–æ–≥—Ç–µ—Ç–æ—á–∫–∞ —Ö–æ—Ä–æ—à–∞—è, –Ω–æ –≤–æ—Ç —Ç–∫–∞–Ω—å –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>–º–Ω–æ–≥–æ –∑–∞—Ç—è–∂–µ–∫, –Ω–µ –ø–æ—Ä–∞–¥–æ–≤–∞–ª–∞ –ø–æ–∫—É–ø–∫–∞</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>—Ä–µ–∫–æ–º–µ–Ω–¥—É—é üí£</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66944</th>\n",
       "      <td>71995</td>\n",
       "      <td>–ø–∞—á–∫–∞ –∫–∞–∫ –ø–∞—á–∫–∞ –∞ –≤–Ω—É—Ç—Ä–∏ —Å–æ–≤—Å–µ–º –¥—Ä—É–≥–æ–µ –Ω–µ–∫—Ç–æ—Ä–≤...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>–≥–æ–≤–Ω–æ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66945</th>\n",
       "      <td>71996</td>\n",
       "      <td>–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ!!! —á–µ—Ä–µ–∑ –≥–æ–¥ —Ç—Ä—è–ø–∫–∞, —É...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>–≥‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66946</th>\n",
       "      <td>71997</td>\n",
       "      <td>–≤–µ—Å 100–≥—Ä, –Ω–µ –ø–æ–Ω–∏–º–∞—é –æ—Ç–∫—É–¥–∞ —Ö–æ—Ä–æ—à–∏–µ –æ—Ç–∑—ã–≤—ã , ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>–≥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66947</th>\n",
       "      <td>71998</td>\n",
       "      <td>–ø—Ä–∏–æ–±—Ä–µ–ª –∏ —É—Å—Ç–∞–Ω–æ–≤–∏–ª —Ä–∞–¥–∏–∞—Ç–æ—Ä –≥–æ–¥ –Ω–∞–∑–∞–¥ , –∞–≤—Ç–æ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>–≥–æ–≤–Ω–æ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66948</th>\n",
       "      <td>71999</td>\n",
       "      <td>–∑–∞–µ–±–∏—Å—å —Ä—É—á–∫–∞, –ø—Ä–∞–≤–¥–∞ –Ω–µ –ø–∏—à–µ—Ç(</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>–∑–∞–µ–±–∏—Å—å</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66949 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                               text  \\\n",
       "0          0                                   —Ö–æ—Ä–æ—à–∏–π, –ø–æ–¥–æ—à–∫–ª   \n",
       "1          1  —Å–æ–≤—Å–µ–º —Ç–æ–Ω—é—Å–µ–Ω—å–∫–∏–π —Å–∞–∂–µ–Ω–µ—Ü, –Ω–µ –¥–æ—Å–º–æ—Ç—Ä–µ–ª–∞ –≤ –æ–ø...   \n",
       "2          2  –∫–æ–≥—Ç–µ—Ç–æ—á–∫–∞ —Ö–æ—Ä–æ—à–∞—è, –Ω–æ –≤–æ—Ç —Ç–∫–∞–Ω—å –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ ...   \n",
       "3          3               –º–Ω–æ–≥–æ –∑–∞—Ç—è–∂–µ–∫, –Ω–µ –ø–æ—Ä–∞–¥–æ–≤–∞–ª–∞ –ø–æ–∫—É–ø–∫–∞   \n",
       "4          4                                       —Ä–µ–∫–æ–º–µ–Ω–¥—É—é üí£   \n",
       "...      ...                                                ...   \n",
       "66944  71995  –ø–∞—á–∫–∞ –∫–∞–∫ –ø–∞—á–∫–∞ –∞ –≤–Ω—É—Ç—Ä–∏ —Å–æ–≤—Å–µ–º –¥—Ä—É–≥–æ–µ –Ω–µ–∫—Ç–æ—Ä–≤...   \n",
       "66945  71996  –æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ!!! —á–µ—Ä–µ–∑ –≥–æ–¥ —Ç—Ä—è–ø–∫–∞, —É...   \n",
       "66946  71997  –≤–µ—Å 100–≥—Ä, –Ω–µ –ø–æ–Ω–∏–º–∞—é –æ—Ç–∫—É–¥–∞ —Ö–æ—Ä–æ—à–∏–µ –æ—Ç–∑—ã–≤—ã , ...   \n",
       "66947  71998  –ø—Ä–∏–æ–±—Ä–µ–ª –∏ —É—Å—Ç–∞–Ω–æ–≤–∏–ª —Ä–∞–¥–∏–∞—Ç–æ—Ä –≥–æ–¥ –Ω–∞–∑–∞–¥ , –∞–≤—Ç–æ...   \n",
       "66948  71999                    –∑–∞–µ–±–∏—Å—å —Ä—É—á–∫–∞, –ø—Ä–∞–≤–¥–∞ –Ω–µ –ø–∏—à–µ—Ç(   \n",
       "\n",
       "                                               label_new    label  \n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           \n",
       "...                                                  ...      ...  \n",
       "66944  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    –≥–æ–≤–Ω–æ  \n",
       "66945  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       –≥‚Ä¶  \n",
       "66946  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     –≥...  \n",
       "66947  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    –≥–æ–≤–Ω–æ  \n",
       "66948  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  –∑–∞–µ–±–∏—Å—å  \n",
       "\n",
       "[66949 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'] = extract_words_from_mask(df_test)\n",
    "\n",
    "df_test.to_csv('/kaggle/working/out_with_words.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c2aa857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:52:38.240404Z",
     "iopub.status.busy": "2024-12-03T14:52:38.240071Z",
     "iopub.status.idle": "2024-12-03T14:52:38.285186Z",
     "shell.execute_reply": "2024-12-03T14:52:38.284545Z"
    },
    "papermill": {
     "duration": 0.310422,
     "end_time": "2024-12-03T14:52:38.286901",
     "exception": false,
     "start_time": "2024-12-03T14:52:37.976479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.drop(columns=['label_new', 'text']).set_index(\"ID\").to_csv(\"/kaggle/working/final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fbf0d9",
   "metadata": {
    "papermill": {
     "duration": 0.314886,
     "end_time": "2024-12-03T14:52:38.866546",
     "exception": false,
     "start_time": "2024-12-03T14:52:38.551660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10312867,
     "sourceId": 88517,
     "sourceType": "competition"
    },
    {
     "datasetId": 6200792,
     "sourceId": 10061944,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 181673,
     "modelInstanceId": 159296,
     "sourceId": 186840,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2413.639643,
   "end_time": "2024-12-03T14:52:42.809675",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-03T14:12:29.170032",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
