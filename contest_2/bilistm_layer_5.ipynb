{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3dafb8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-04T06:11:38.345638Z",
     "iopub.status.busy": "2024-12-04T06:11:38.345006Z",
     "iopub.status.idle": "2024-12-04T06:11:40.307070Z",
     "shell.execute_reply": "2024-12-04T06:11:40.306142Z"
    },
    "papermill": {
     "duration": 1.969503,
     "end_time": "2024-12-04T06:11:40.308936",
     "exception": false,
     "start_time": "2024-12-04T06:11:38.339433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key='b123af3ff1bc7e54569d0976c6405a5b3b6d2902')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07344bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T06:11:40.318373Z",
     "iopub.status.busy": "2024-12-04T06:11:40.317805Z",
     "iopub.status.idle": "2024-12-04T06:11:45.126769Z",
     "shell.execute_reply": "2024-12-04T06:11:45.126069Z"
    },
    "papermill": {
     "duration": 4.815668,
     "end_time": "2024-12-04T06:11:45.128698",
     "exception": false,
     "start_time": "2024-12-04T06:11:40.313030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9303824a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T06:11:45.139063Z",
     "iopub.status.busy": "2024-12-04T06:11:45.138584Z",
     "iopub.status.idle": "2024-12-04T06:11:45.147717Z",
     "shell.execute_reply": "2024-12-04T06:11:45.146869Z"
    },
    "papermill": {
     "duration": 0.01581,
     "end_time": "2024-12-04T06:11:45.149356",
     "exception": false,
     "start_time": "2024-12-04T06:11:45.133546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text to retain only alphabetic characters and convert to lowercase.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): Input string.\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed string.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def all_preprocessing(df):\n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "def extract_words_from_mask(df, text_col='text', mask_col='label_new'):\n",
    "    \"\"\"\n",
    "    Extracts substrings from `text` based on the boolean mask in `label_new`.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe with columns `text` and `label_new`.\n",
    "        text_col (str): Name of the column containing the text.\n",
    "        mask_col (str): Name of the column containing the boolean mask.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: A Series where each entry is a list of words extracted from `text`.\n",
    "    \"\"\"\n",
    "    def process_row(row):\n",
    "        text = row[text_col]\n",
    "        mask = row[mask_col]\n",
    "        words = []\n",
    "        current_word = []\n",
    "        \n",
    "        for char, include in zip(text, mask):\n",
    "            if include:\n",
    "                current_word.append(char)\n",
    "            elif current_word:  # If a word is in progress and we hit a `0`\n",
    "                words.append(\"\".join(current_word))\n",
    "                current_word = []  # Reset for the next word\n",
    "        \n",
    "        if current_word:  # Append the last word if still in progress\n",
    "            words.append(\"\".join(current_word))\n",
    "        words = sorted(words)\n",
    "        return \",\".join(words)\n",
    "    \n",
    "    return df.apply(process_row, axis=1)\n",
    "\n",
    "\n",
    "def to_out_labels(texts, labels):\n",
    "    def process_row(i):\n",
    "        text = texts[i]\n",
    "        mask = labels[i]\n",
    "        words = []\n",
    "        current_word = []\n",
    "        \n",
    "        for char, include in zip(text, mask):\n",
    "            if include:\n",
    "                current_word.append(char)\n",
    "            elif current_word:  # If a word is in progress and we hit a `0`\n",
    "                words.append(\"\".join(current_word))\n",
    "                current_word = []  # Reset for the next word\n",
    "        \n",
    "        if current_word:  # Append the last word if still in progress\n",
    "            words.append(\"\".join(current_word))\n",
    "\n",
    "        words = sorted(words)\n",
    "        return \",\".join(words)\n",
    "    out_labels = [process_row(i) for i in range(len(texts))]\n",
    "    return out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b849de9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T06:11:45.159066Z",
     "iopub.status.busy": "2024-12-04T06:11:45.158762Z",
     "iopub.status.idle": "2024-12-04T06:12:20.571777Z",
     "shell.execute_reply": "2024-12-04T06:12:20.570857Z"
    },
    "papermill": {
     "duration": 35.424835,
     "end_time": "2024-12-04T06:12:20.577872",
     "exception": false,
     "start_time": "2024-12-04T06:11:45.153037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248088</th>\n",
       "      <td>–º–Ω–µ –µ–µ –ø–æ—Ä–≤–∞–ª–∏ —Å—É–∫–∏</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248089</th>\n",
       "      <td>–ø–æ–ª–Ω–æ–µ –¥–µ—Ä—å–º–æ, —É–¥–∞–ª–∏—Ç–µ —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä –∏ –∑–∞–±–ª–æ–∫–∏—Ä—É–π...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248090</th>\n",
       "      <td>—Ö–µ—Ä–Ω—è. –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä.</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248091</th>\n",
       "      <td>—ç—Ç–æ –≤–æ–æ–±—â–µ —á—Ç–æ , –∑–∞ üí© –≥–æ–≤... —â–µ?? —Ç–µ–º–Ω–æ—Ç–∏—â–∞ —É–∂...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248092</th>\n",
       "      <td>–Ω–µ –±–µ—Ä–∏—Ç–µ!!!!! –º–µ–ª–∫–∏–µ, –ø–æ—Ä–µ–∑–∞–Ω–Ω—ã–µ, –ø–æ–¥–ø–æ—Ä—á–µ–Ω–Ω—ã...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "ID                                                          \n",
       "248088                                –º–Ω–µ –µ–µ –ø–æ—Ä–≤–∞–ª–∏ —Å—É–∫–∏   \n",
       "248089  –ø–æ–ª–Ω–æ–µ –¥–µ—Ä—å–º–æ, —É–¥–∞–ª–∏—Ç–µ —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä –∏ –∑–∞–±–ª–æ–∫–∏—Ä—É–π...   \n",
       "248090                            —Ö–µ—Ä–Ω—è. –¥–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä.   \n",
       "248091  —ç—Ç–æ –≤–æ–æ–±—â–µ —á—Ç–æ , –∑–∞ üí© –≥–æ–≤... —â–µ?? —Ç–µ–º–Ω–æ—Ç–∏—â–∞ —É–∂...   \n",
       "248092  –Ω–µ –±–µ—Ä–∏—Ç–µ!!!!! –º–µ–ª–∫–∏–µ, –ø–æ—Ä–µ–∑–∞–Ω–Ω—ã–µ, –ø–æ–¥–ø–æ—Ä—á–µ–Ω–Ω—ã...   \n",
       "\n",
       "                                                    label  \n",
       "ID                                                         \n",
       "248088  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "248089  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, ...  \n",
       "248090  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "248091  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "248092  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/wb-contest-2/wb_contest_2_new_dataset.csv', index_col='ID')\n",
    "df['label'] = df['label'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe8abbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T06:12:20.586924Z",
     "iopub.status.busy": "2024-12-04T06:12:20.586635Z",
     "iopub.status.idle": "2024-12-04T06:12:20.598347Z",
     "shell.execute_reply": "2024-12-04T06:12:20.597462Z"
    },
    "papermill": {
     "duration": 0.018071,
     "end_time": "2024-12-04T06:12:20.599891",
     "exception": false,
     "start_time": "2024-12-04T06:12:20.581820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom Tokenizer: Split into characters and map each character to an index\n",
    "class CharTokenizer:\n",
    "    def __init__(self, texts):\n",
    "        # Create a vocabulary from the unique characters in the dataset + a padding token (0)\n",
    "        self.vocab = {char: idx + 1 for idx, char in enumerate(set(''.join(texts)))}  # index 0 is for padding\n",
    "        self.vocab_size = len(self.vocab) + 1  # Add 1 for padding\n",
    "        self.vocab['<PAD>'] = 0  # Adding padding token\n",
    "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Return the indices of each character in the text based on the vocabulary\n",
    "        return [self.vocab.get(char, self.vocab['<PAD>']) for char in text]\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        # Decode token IDs back to characters\n",
    "        return ''.join([self.reverse_vocab.get(idx, '<PAD>') for idx in token_ids])\n",
    "\n",
    "# Custom Dataset\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=1024):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text into character-level tokens\n",
    "        input_ids = self.tokenizer.encode(text)\n",
    "        \n",
    "        # Pad the sequence to max_len if necessary\n",
    "        input_ids = input_ids + [0] * (self.max_len - len(input_ids)) if len(input_ids) < self.max_len else input_ids[:self.max_len]\n",
    "        \n",
    "        # Pad the labels to max_len if necessary\n",
    "        labels_padded = label + [0] * (self.max_len - len(label)) if len(label) < self.max_len else label[:self.max_len]\n",
    "        #print(sum(labels_padded))\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([1] * len(input_ids) + [0] * (self.max_len - len(input_ids)), dtype=torch.long),  # Attention mask\n",
    "            'labels': torch.tensor(labels_padded, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=1024):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(text)\n",
    "        input_ids = input_ids + [0] * (self.max_len - len(input_ids)) if len(input_ids) < self.max_len else input_ids[:self.max_len]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([1] * len(input_ids) + [0] * (self.max_len - len(input_ids)), dtype=torch.long)  # Attention mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede2dcc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T06:12:20.608409Z",
     "iopub.status.busy": "2024-12-04T06:12:20.608152Z",
     "iopub.status.idle": "2024-12-04T06:12:20.613762Z",
     "shell.execute_reply": "2024-12-04T06:12:20.612926Z"
    },
    "papermill": {
     "duration": 0.011767,
     "end_time": "2024-12-04T06:12:20.615439",
     "exception": false,
     "start_time": "2024-12-04T06:12:20.603672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model for token classification (character-level)\n",
    "class CharTokenClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_dim=256, num_classes=2, num_layers=5, bidirectional=True, dropout=0.2):\n",
    "        super(CharTokenClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Character embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(lstm_dim * 2 if bidirectional else lstm_dim, num_classes)  # Adjusted for bidirectional\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get embeddings for each character token\n",
    "        x = self.embedding(input_ids)  # (batch_size, seq_len, embedding_dim)\n",
    "        x, (hn, cn) = self.lstm(x)  # (batch_size, seq_len, hidden_dim)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)  # (batch_size, seq_len, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a4f0904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T06:12:20.624456Z",
     "iopub.status.busy": "2024-12-04T06:12:20.623767Z",
     "iopub.status.idle": "2024-12-04T06:12:21.794519Z",
     "shell.execute_reply": "2024-12-04T06:12:21.793554Z"
    },
    "papermill": {
     "duration": 1.17696,
     "end_time": "2024-12-04T06:12:21.796253",
     "exception": false,
     "start_time": "2024-12-04T06:12:20.619293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1131\n"
     ]
    }
   ],
   "source": [
    "texts = df['text'].values  # or df['text'].to_numpy()\n",
    "labels = df['label'].values  # or df['label'].to_numpy()\n",
    "\n",
    "tokenizer = CharTokenizer(texts)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = TokenClassificationDataset(texts, labels, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0ec0391",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T06:12:21.805936Z",
     "iopub.status.busy": "2024-12-04T06:12:21.805165Z",
     "iopub.status.idle": "2024-12-04T11:04:01.158557Z",
     "shell.execute_reply": "2024-12-04T11:04:01.157640Z"
    },
    "papermill": {
     "duration": 17499.36001,
     "end_time": "2024-12-04T11:04:01.160366",
     "exception": false,
     "start_time": "2024-12-04T06:12:21.800356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [58:15<00:00,  1.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [58:21<00:00,  1.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [58:20<00:00,  1.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [58:19<00:00,  1.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3825/3825 [58:20<00:00,  1.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CharTokenClassifier(vocab_size=tokenizer.vocab_size, embedding_dim=16)\n",
    "loss_fn = nn.CrossEntropyLoss()  # For token classification (binary)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "        # Get inputs and labels\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits.view(-1, 2), labels.view(-1))  # Flatten the logits and labels\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29da4098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:04:02.986950Z",
     "iopub.status.busy": "2024-12-04T11:04:02.986076Z",
     "iopub.status.idle": "2024-12-04T11:04:03.051702Z",
     "shell.execute_reply": "2024-12-04T11:04:03.051048Z"
    },
    "papermill": {
     "duration": 0.9739,
     "end_time": "2024-12-04T11:04:03.053699",
     "exception": false,
     "start_time": "2024-12-04T11:04:02.079799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/kaggle/working/model.pth')\n",
    "\n",
    "import pickle\n",
    "with open('/kaggle/working/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df37e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:04:04.813130Z",
     "iopub.status.busy": "2024-12-04T11:04:04.812782Z",
     "iopub.status.idle": "2024-12-04T11:04:05.545377Z",
     "shell.execute_reply": "2024-12-04T11:04:05.544231Z"
    },
    "papermill": {
     "duration": 1.630253,
     "end_time": "2024-12-04T11:04:05.546635",
     "exception": true,
     "start_time": "2024-12-04T11:04:03.916382",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CharTokenClassifier:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([1131, 16]) from checkpoint, the shape in current model is torch.Size([1131, 32]).\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for lstm.weight_ih_l0_reverse: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the model, loss function, and optimizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m CharTokenClassifier(vocab_size\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mvocab_size, embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CharTokenClassifier:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([1131, 16]) from checkpoint, the shape in current model is torch.Size([1131, 32]).\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for lstm.weight_ih_l0_reverse: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32])."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CharTokenClassifier(vocab_size=tokenizer.vocab_size, embedding_dim=32)\n",
    "model.load_state_dict(torch.load('/kaggle/working/model.pth', weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to run inference on the entire dataset or a sample\n",
    "def run_eval(dataloader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for batch in tqdm(dataloader, desc=\"Running Inference\"):\n",
    "            # Move batch data to the correct device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Run the model on the batch\n",
    "            logits = model(input_ids, attention_mask)  # (batch_size, seq_len, num_classes)\n",
    "\n",
    "            # Apply softmax to get class probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # Shape: (batch_size, seq_len, num_classes)\n",
    "\n",
    "            # Get predicted class for each token\n",
    "            predicted_class = torch.argmax(probs, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            for i in range(len(labels)):\n",
    "                all_preds.append(predicted_class[i].cpu().numpy())  # Move to CPU for easy handling\n",
    "                all_labels.append(labels[i].cpu().numpy())\n",
    "\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Run inference\n",
    "#predictions, ground_truth = run_eval(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ceda9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T13:10:38.529594Z",
     "iopub.status.busy": "2024-12-03T13:10:38.529202Z",
     "iopub.status.idle": "2024-12-03T13:10:43.250779Z",
     "shell.execute_reply": "2024-12-03T13:10:43.249727Z",
     "shell.execute_reply.started": "2024-12-03T13:10:38.529561Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "i = 244736\n",
    "text = dataset.texts[i]\n",
    "# Example: Print the first prediction and corresponding ground truth\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Prediction: {predictions[i]}\")\n",
    "print(f\"Ground truth: {ground_truth[i]}\")\n",
    "\n",
    "predicted = pd.DataFrame({\"text\": dataset.texts, \"prediction\": predictions, \"label\": ground_truth})\n",
    "predicted['label_out'] = extract_words_from_mask(predicted, mask_col='prediction')\n",
    "\n",
    "predicted\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dbb7d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T13:10:43.252549Z",
     "iopub.status.busy": "2024-12-03T13:10:43.252237Z",
     "iopub.status.idle": "2024-12-03T13:11:03.368622Z",
     "shell.execute_reply": "2024-12-03T13:11:03.367831Z",
     "shell.execute_reply.started": "2024-12-03T13:10:43.252523Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predicted.to_csv(\"/kaggle/working/predicted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc6e2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:43:34.209236Z",
     "iopub.status.busy": "2024-12-04T05:43:34.208884Z",
     "iopub.status.idle": "2024-12-04T05:43:36.061318Z",
     "shell.execute_reply": "2024-12-04T05:43:36.060268Z",
     "shell.execute_reply.started": "2024-12-04T05:43:34.209205Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CharTokenClassifier(vocab_size=tokenizer.vocab_size, embedding_dim=16)\n",
    "model.load_state_dict(torch.load('/kaggle/input/layer-5/pytorch/default/1/model.pth', weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#with open('/kaggle/working/tokenizer.pickle', 'rb') as handle:\n",
    "#    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a9ab05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:43:36.063294Z",
     "iopub.status.busy": "2024-12-04T05:43:36.062896Z",
     "iopub.status.idle": "2024-12-04T05:43:37.150674Z",
     "shell.execute_reply": "2024-12-04T05:43:37.149715Z",
     "shell.execute_reply.started": "2024-12-04T05:43:36.063254Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/kaggle/input/wildberries-winter-school-24-contest-detected/test.csv')\n",
    "df_test = all_preprocessing(df_test)\n",
    "\n",
    "texts = df_test['text'].values  # or df['text'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c9f31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:43:37.154841Z",
     "iopub.status.busy": "2024-12-04T05:43:37.154058Z",
     "iopub.status.idle": "2024-12-04T05:58:32.282739Z",
     "shell.execute_reply": "2024-12-04T05:58:32.281666Z",
     "shell.execute_reply.started": "2024-12-04T05:43:37.154806Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_test = InferenceDataset(texts, tokenizer)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=False)\n",
    "\n",
    "def run_inference(model, dataloader, device):\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for batch in tqdm(dataloader, desc=\"Running Inference\"):\n",
    "            # Move batch data to the correct device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Run the model on the batch\n",
    "            logits = model(input_ids, attention_mask)  # (batch_size, seq_len, num_classes)\n",
    "\n",
    "            # Apply softmax to get class probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # Shape: (batch_size, seq_len, num_classes)\n",
    "\n",
    "            # Get predicted class for each token\n",
    "            predicted_class = torch.argmax(probs, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            for i in range(len(predicted_class)):\n",
    "                all_preds.append(predicted_class[i].cpu().numpy())  # Move to CPU for easy handling\n",
    "\n",
    "    return all_preds\n",
    "\n",
    "# Run inference\n",
    "predictions = run_inference(model, dataloader_test, device)\n",
    "\n",
    "df_test[\"label_new\"] = predictions\n",
    "df_test.to_csv('/kaggle/working/out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18403cca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:58:32.283988Z",
     "iopub.status.busy": "2024-12-04T05:58:32.283704Z",
     "iopub.status.idle": "2024-12-04T05:58:36.995296Z",
     "shell.execute_reply": "2024-12-04T05:58:36.994338Z",
     "shell.execute_reply.started": "2024-12-04T05:58:32.283962Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test['label'] = extract_words_from_mask(df_test)\n",
    "\n",
    "df_test.to_csv('/kaggle/working/out_with_words.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48a6cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:58:36.996502Z",
     "iopub.status.busy": "2024-12-04T05:58:36.996254Z",
     "iopub.status.idle": "2024-12-04T05:58:37.042562Z",
     "shell.execute_reply": "2024-12-04T05:58:37.041550Z",
     "shell.execute_reply.started": "2024-12-04T05:58:36.996479Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.drop(columns=['label_new', 'text']).set_index(\"ID\").to_csv(\"/kaggle/working/final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715bb37",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10312867,
     "sourceId": 88517,
     "sourceType": "competition"
    },
    {
     "datasetId": 6200792,
     "sourceId": 10061944,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 182426,
     "modelInstanceId": 160050,
     "sourceId": 187735,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17552.467945,
   "end_time": "2024-12-04T11:04:08.380891",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-04T06:11:35.912946",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
